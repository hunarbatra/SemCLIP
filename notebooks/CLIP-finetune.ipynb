{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hunar/SemCLIP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hunar/SemCLIP/svt-llava-env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt\n",
    "# !pip3 install supervision torch tqdm fire datasets opencv-python openai-clip huggingface-hub torch python-dotenv clip torchvision Pillow pandas numpy matplotlib transformers\n",
    "# !pip install --upgrade notebook\n",
    "# !pip install --upgrade ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash sam_model_setup.sh\n",
    "# !pip3 install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from SemCLIP.semclip import SemCLIP\n",
    "from SemCLIP.image_utils import DEVICE, create_batches, pil_to_cv2\n",
    "from SemCLIP.model_utils import convert_models_to_fp32, convert_models_to_fp16\n",
    "\n",
    "\n",
    "# semclip = SemCLIP(model_name=\"openai/clip-vit-base-patch32\", pool_type='attention', projection_dim=512, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "from config import dataset_mapper\n",
    "\n",
    "dataset = load_dataset(dataset_mapper['COCO-13k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "from SemCLIP.semclip import SemCLIP\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Model without downloading images locally [with HF data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bvwr2qt5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbf485214a7448cbde7f41a76dc110a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>▃▅▃▇▅▂▆▃█▃█▂▄▂▂▄▅▄▄▄▄▂▃▂▄▃▂▂▂▂▃▃▄▃▃▃▁▅▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>0.28157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clip-v1-test-64</strong> at: <a href='https://wandb.ai/hunarbatra/semclip/runs/bvwr2qt5' target=\"_blank\">https://wandb.ai/hunarbatra/semclip/runs/bvwr2qt5</a><br/> View project at: <a href='https://wandb.ai/hunarbatra/semclip' target=\"_blank\">https://wandb.ai/hunarbatra/semclip</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240718_215853-bvwr2qt5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bvwr2qt5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd8a88933514cf1b0c75fe9a87b854e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113395800607072, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hunar/SemCLIP/wandb/run-20240718_220235-9nyn1m1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hunarbatra/semclip/runs/9nyn1m1o' target=\"_blank\">clip-v1-test-64</a></strong> to <a href='https://wandb.ai/hunarbatra/semclip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hunarbatra/semclip' target=\"_blank\">https://wandb.ai/hunarbatra/semclip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hunarbatra/semclip/runs/9nyn1m1o' target=\"_blank\">https://wandb.ai/hunarbatra/semclip/runs/9nyn1m1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        image = self.processor(images=example['downloaded_img'], return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        caption = self.processor.tokenizer(example['caption'], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=77)['input_ids'].squeeze(0)\n",
    "        return image, caption\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"hunarbatra/CLIP-LLaVA-Instruct-COCO-13k\")\n",
    "train_dataset = ImageCaptionDataset(dataset['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-6 # or 1e-7\n",
    "betas = (0.9, 0.98)\n",
    "epsilon = 1e-6\n",
    "weight_decay = 0.001 # originally 0.2\n",
    "num_epochs = 1\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "wandb_config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"betas\": betas,\n",
    "    \"epsilon\": epsilon,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "\n",
    "train_name = 'clip-v1-test-64'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas, eps=epsilon, weight_decay=weight_decay)\n",
    "\n",
    "wandb.init(project=\"semclip\", name=train_name, config=wandb_config)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images, captions = batch\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        outputs = model(pixel_values=images, input_ids=captions)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        \n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        \n",
    "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "        total_loss.backward()\n",
    "        \n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            model.to(device)\n",
    "            convert_models_to_fp16(clip_model)\n",
    "        \n",
    "        # Log the loss to wandb\n",
    "        wandb.log({\"Loss\": total_loss.item()})\n",
    "        \n",
    "        # Update the progress bar description with the current loss\n",
    "        progress_bar.set_postfix({\"Loss\": total_loss.item()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "/home/hunar/SemCLIP/clip-v1-test is already a clone of https://huggingface.co/hunarbatra/clip-v1-test. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0658d0ca70eb4e9f8688c26f84c8ac5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model.safetensors:   0%|          | 1.00/577M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/hunarbatra/clip-v1-test\n",
      "   143eeef..47d3c3b  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, Repository\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "def upload_model_to_hf_hub(model, processor, model_name, hf_name):\n",
    "    api = HfApi()\n",
    "\n",
    "    repo_exists = api.repo_exists(repo_id=f\"{hf_name}/{model_name}\", token=HF_TOKEN)\n",
    "    print(f\"Repository exists: {repo_exists}\")\n",
    "\n",
    "    if repo_exists:\n",
    "        repo = Repository(local_dir=model_name, clone_from=f\"https://huggingface.co/{hf_name}/{model_name}\", token=HF_TOKEN)\n",
    "        commit_message = \"Update model files\"\n",
    "    else:\n",
    "        repo_url = api.create_repo(repo_id=model_name, token=HF_TOKEN, private=True)\n",
    "        repo = Repository(local_dir=model_name, clone_from=repo_url, use_auth_token=HF_TOKEN)\n",
    "        commit_message = \"Add model files\"\n",
    "\n",
    "    model.save_pretrained(model_name)\n",
    "    model.config.save_pretrained(model_name)\n",
    "    # tokenizer.save_pretrained(model_name)\n",
    "    processor.save_pretrained(model_name)\n",
    "    \n",
    "    repo.push_to_hub(commit_message=commit_message)\n",
    "        \n",
    "upload_model_to_hf_hub(model, processor, model_name='clip-v1-test', hf_name='hunarbatra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|█████████████████████| 605M/605M [01:21<00:00, 7.42MB/s]\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]Batch 0: Zero-shot top-1 Accuracy: 68.75%, std err: 8.19%\n",
      "Batch 0: Avg Cosine Similarity Score: 29.167416274547577, std err: 0.46%\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.04it/s]Batch 1: Zero-shot top-1 Accuracy: 64.06%, std err: 6.00%\n",
      "Batch 1: Avg Cosine Similarity Score: 28.98438537120819, std err: 0.32%\n",
      "2it [00:00,  2.66it/s]                                                          Batch 2: Zero-shot top-1 Accuracy: 66.67%, std err: 4.81%\n",
      "Batch 2: Avg Cosine Similarity Score: 29.058409055074055, std err: 0.26%\n",
      "3it [00:01,  2.98it/s]Batch 3: Zero-shot top-1 Accuracy: 67.00%, std err: 4.70%\n",
      "Batch 3: Avg Cosine Similarity Score: 28.966181106567383, std err: 0.26%\n",
      "4it [00:01,  3.50it/s]\n",
      " Zero-shot top-1 Accuracy: 67.00%, std err: 4.70%\n",
      " Avg Cosine Similarity Score: 28.966181106567383, std err: 0.26%\n"
     ]
    }
   ],
   "source": [
    "!python3 evaluate.py --model_name='clip-v1-test' --data='COCO-13k' --batch_size=32 --data_subset=100 --eval_run_name='test6' --max_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]Batch 0: Zero-shot top-1 Accuracy: 65.62%, std err: 8.40%\n",
      "Batch 0: Avg Cosine Similarity Score: 29.700043380260468, std err: 0.53%\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.05it/s]Batch 1: Zero-shot top-1 Accuracy: 59.38%, std err: 6.14%\n",
      "Batch 1: Avg Cosine Similarity Score: 29.69073587656021, std err: 0.40%\n",
      "2it [00:00,  2.67it/s]                                                          Batch 2: Zero-shot top-1 Accuracy: 62.50%, std err: 4.94%\n",
      "Batch 2: Avg Cosine Similarity Score: 29.620400329430897, std err: 0.32%\n",
      "3it [00:01,  3.02it/s]Batch 3: Zero-shot top-1 Accuracy: 63.00%, std err: 4.83%\n",
      "Batch 3: Avg Cosine Similarity Score: 29.574338397979737, std err: 0.32%\n",
      "4it [00:01,  3.54it/s]\n",
      " Zero-shot top-1 Accuracy: 63.00%, std err: 4.83%\n",
      " Avg Cosine Similarity Score: 29.574338397979737, std err: 0.32%\n"
     ]
    }
   ],
   "source": [
    "!python3 evaluate.py --model_name='clip' --data='COCO-13k' --batch_size=32 --data_subset=100 --eval_run_name='test6' --max_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
